import torch
import gpytorch
from gpytorch import settings as gsettings
from linear_operator.utils import linear_cg
from linear_operator.operators import IdentityLinearOperator
from .kernels.base import BaseGRFKernel


class GraphGP(gpytorch.models.ExactGP):
    def __init__(self, x_train, y_train, likelihood, kernel: BaseGRFKernel):
        super().__init__(x_train, y_train, likelihood)
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = kernel

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

    def predict(self, x_test, n_samples=64):
        train_indices = self.x_train.int().flatten()
        test_indices = x_test.int().flatten()

        phi = self.covar_module._get_feature_matrix()
        phi_train = phi[train_indices, :]
        phi_test = phi[test_indices, :]

        K_train_train = phi_train @ phi_train.T
        K_test_train = phi_test @ phi_train.T

        noise_variance = self.likelihood.noise.item()
        noise_std = torch.sqrt(torch.tensor(noise_variance, device=x_test.device))
        A = K_train_train + noise_variance * IdentityLinearOperator(
            len(train_indices), device=x_test.device
        )

        eps1_batch = torch.randn(n_samples, self.num_nodes, device=x_test.device)
        eps2_batch = noise_std * torch.randn(
            n_samples, len(train_indices), device=x_test.device
        )

        f_test_prior = eps1_batch @ phi_test.T
        f_train_prior = eps1_batch @ phi_train.T

        b_batch = self.y_train.unsqueeze(0) - (f_train_prior + eps2_batch)
        v_batch = linear_cg(
            A._matmul, b_batch.T, tolerance=gsettings.cg_tolerance.value()
        )

        return f_test_prior + (K_test_train @ v_batch).T
